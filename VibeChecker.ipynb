{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOAL: notifications + divergence notifications\n",
    "------------------------------------------------------------------------------\n",
    "- Bollinger Band............. yes\n",
    "- RSI........................ yes\n",
    "- RSI Divergence............. no\n",
    "- MACD....................... yes\n",
    "- MVWAP....................... no\n",
    "- RVI........................ yes\n",
    "- MFI........................ no\n",
    "- Min & Max lines............ yes\n",
    "- Moving Averages............ no\n",
    "- Candlestick Patterns....... yes\n",
    "- Risk Analysis.............. no\n",
    "- Earnings & Financials...... no\n",
    "- News....................... yes\n",
    "- Put/Call Ratio............. no\n",
    "- Trending................... no\n",
    "- Consolodating.............. no\n",
    "- Buy Score:   Some #/#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing variables\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from openpyxl import load_workbook\n",
    "# Find local peaks\n",
    "import matplotlib.dates as mdates \n",
    "from scipy.signal import argrelextrema\n",
    "import yfinance as yf\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import Request\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pythoncom\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothing with Wilder puts emphasis on recent values\n",
    "def Wilder(data, periods):\n",
    "    start = np.where(~np.isnan(data))[0][0] #Check if nans present in beginning\n",
    "    Wilder = np.array([np.nan]*len(data))\n",
    "    Wilder[start+periods-1] = data[start:(start+periods)].mean() #Simple Moving Average\n",
    "    for i in range(start+periods,len(data)):\n",
    "        Wilder[i] = (Wilder[i-1]*(periods-1) + data[i])/periods #Wilder Smoothing\n",
    "    return(Wilder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import data for the year of ticker(s)\n",
    "all_data = pd.DataFrame()\n",
    "test_data = pd.DataFrame()\n",
    "no_data = []\n",
    "tickers_final = ['SPY']\n",
    "for i in tickers_final:\n",
    "    all_data = yf.download(tickers=i, period = '10y', interval = '1d')\n",
    "    all_data['symbol'] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bollinger Band parameters\n",
    "window = 20\n",
    "num_std_dev = 2\n",
    "\n",
    "# Calculate Middle Band\n",
    "all_data['middle_band'] = all_data['Close'].rolling(window=window).mean()\n",
    "\n",
    "# Calculate Standard Deviation\n",
    "all_data['std_dev'] = all_data['Close'].rolling(window=window).std()\n",
    "\n",
    "# Calculate Upper and Lower Bands\n",
    "all_data['upper_band'] = all_data['middle_band'] + (all_data['std_dev'] * num_std_dev)\n",
    "all_data['lower_band'] = all_data['middle_band'] - (all_data['std_dev'] * num_std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of close position between bands\n",
    "all_data['bollinger_position_score'] = (all_data['Close'] - all_data['lower_band']) / (all_data['upper_band'] - all_data['lower_band'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MACD Calculations: check if it is higher than the signal (bullish)\n",
    "all_data['ShortEMA'] = all_data.Close.transform(lambda x: x.ewm(span=5, adjust=False).mean())\n",
    "all_data['LongEMA'] = all_data.Close.transform(lambda x: x.ewm(span=35, adjust=False).mean())\n",
    "all_data['MACD'] = all_data.ShortEMA - all_data.LongEMA\n",
    "all_data['signal'] = all_data.MACD.transform(lambda x: x.ewm(span=5, adjust=False).mean())\n",
    "all_data['macd_bullish'] = all_data.apply(lambda x : 1 if (x['MACD'] - x['signal'] > 0) else 0, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ATR and ADX are used to determine signal strength\n",
    "# ATR ratio and ADX is used to figure out a true range, for volatility\n",
    "all_data['prev_close'] = all_data.groupby('symbol')['Close'].shift(1)\n",
    "all_data['TR'] = np.maximum((all_data['High'] - all_data['Low']), \n",
    "                    np.maximum(abs(all_data['High'] - all_data['prev_close']), \n",
    "                    abs(all_data['prev_close'] - all_data['Low'])))\n",
    "\n",
    "for i in all_data['symbol'].unique():\n",
    "    TR_data = all_data[all_data.symbol == i].copy()\n",
    "    all_data.loc[all_data.symbol==i,'ATR_5'] = Wilder(TR_data['TR'], 5)\n",
    "    all_data.loc[all_data.symbol==i,'ATR_15'] = Wilder(TR_data['TR'], 15)\n",
    "all_data['ATR_Ratio'] = all_data['ATR_5'] / all_data['ATR_15']\n",
    "\n",
    "# Took this code from the internet for ATR and ADX for a different project, along with the Wilder smoothing function\n",
    "all_data['prev_high'] = all_data.groupby('symbol')['High'].shift(1)\n",
    "all_data['prev_low'] = all_data.groupby('symbol')['Low'].shift(1)\n",
    "\n",
    "all_data['+DM'] = np.where(~np.isnan(all_data.prev_high),\n",
    "                        np.where((all_data['High'] > all_data['prev_high']) & \n",
    "        (((all_data['High'] - all_data['prev_high']) > (all_data['prev_low'] - all_data['Low']))), \n",
    "                                                                all_data['High'] - all_data['prev_high'], \n",
    "                                                                0),np.nan)\n",
    "all_data['-DM'] = np.where(~np.isnan(all_data.prev_low),\n",
    "                        np.where((all_data['prev_low'] > all_data['Low']) & \n",
    "        (((all_data['prev_low'] - all_data['Low']) > (all_data['High'] - all_data['prev_high']))), \n",
    "                                    all_data['prev_low'] - all_data['Low'], \n",
    "                                    0),np.nan)\n",
    "\n",
    "for i in all_data['symbol'].unique():\n",
    "    ADX_data = all_data[all_data.symbol == i].copy()\n",
    "    all_data.loc[all_data.symbol==i,'+DM_15'] = Wilder(ADX_data['+DM'], 15)\n",
    "    all_data.loc[all_data.symbol==i,'-DM_15'] = Wilder(ADX_data['-DM'], 15)\n",
    "all_data['+DI_15'] = (all_data['+DM_15']/all_data['ATR_15'])*100\n",
    "all_data['-DI_15'] = (all_data['-DM_15']/all_data['ATR_15'])*100\n",
    "all_data['DX_15'] = (np.round(abs(all_data['+DI_15'] - all_data['-DI_15'])/(all_data['+DI_15'] + all_data['-DI_15']) * 100))\n",
    "\n",
    "for i in all_data['symbol'].unique():\n",
    "    ADX_data = all_data[all_data.symbol == i].copy()\n",
    "    all_data.loc[all_data.symbol==i,'ADX_15'] = Wilder(ADX_data['DX_15'], 15)\n",
    "\n",
    "# Determine if the stock has momentum and volatility (strength of the signal)\n",
    "all_data['adx_signal'] = all_data.apply(lambda x : 1 if (x['ADX_15'] >= 25) else 0, axis = 1)\n",
    "all_data['atr_signal'] = all_data.apply(lambda x : 1 if (x['ATR_Ratio'] >= 1) else 0, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Bollinger Band Width as our range\n",
    "all_data['range'] = all_data['upper_band'] - all_data['lower_band']\n",
    "all_data['avg_range'] = all_data['range'].rolling(window=10).mean()\n",
    "all_data['range_score'] = 1 - (all_data['range'] / all_data['avg_range'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing threshold to start with arbitrarily, can optimize later\n",
    "low_atr_threshold = all_data['ATR_15'].quantile(0.25)\n",
    "all_data['is_consolidating'] = (all_data['ADX_15'] < 20) & (all_data['ATR_15'] < low_atr_threshold)\n",
    "# Casting 'is_consolidating' to bool to ensure compatibility\n",
    "all_data['is_consolidating'] = all_data['is_consolidating'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate duration of stability for duration scoring\n",
    "all_data['consolidation_start_date'] = all_data.index.where(all_data['is_consolidating'] & (~all_data['is_consolidating'].shift(1).fillna(False)))\n",
    "# Forward fill to propagate the start date of the consolidation period forward\n",
    "all_data['consolidation_start_date'] = all_data['consolidation_start_date'].ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate duration of stability\n",
    "all_data['consolidation_duration'] = (all_data.index - all_data['consolidation_start_date']).dt.days # Extracts days from Timedelta\n",
    "# Normalize duration score\n",
    "all_data['duration_score'] = (all_data['consolidation_duration'] - all_data['consolidation_duration'].min()) / (all_data['consolidation_duration'].max() - all_data['consolidation_duration'].min())\n",
    "all_data['duration_score'].fillna(0, inplace=True)  # 0 as default for non-consolidating periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates volume score for different classifications\n",
    "all_data['avg_volume'] = all_data['Volume'].rolling(window=10).mean()\n",
    "all_data['volume_score'] = 1 - (all_data['Volume'] / (all_data['avg_volume'] + 1e-10))\n",
    "# all_data['volume_score'] = all_data['volume_score'].clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Combine scores to form a consolidated score\n",
    "all_data['consolidation_score'] = (all_data['range_score'] * 0.30 +\n",
    "                                   all_data['duration_score'] * 0.13 +\n",
    "                                   all_data['volume_score'] * 0.2 +\n",
    "                                  (all_data['ADX_15'] < 20) * 0.185 +  # Use binary signals for low ADX\n",
    "                                  (all_data['ATR_15'] < low_atr_threshold) * 0.185)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask the zero values in the consolidation score\n",
    "all_data['consolidation_score_plot'] = all_data['consolidation_score']\n",
    "# Calculate a simple moving average of the consolidation score with a window size of 10 days\n",
    "all_data['smoothed_consolidation_score'] = all_data['consolidation_score_plot'].rolling(window=10).mean()\n",
    "\n",
    "# Normalize smoothed score to fit within the range of close prices for better visualization\n",
    "min_close = all_data['Close'].min()\n",
    "max_close = all_data['Close'].max()\n",
    "\n",
    "scaled_smoothed_consolidation_score = ((all_data['smoothed_consolidation_score'] - all_data['smoothed_consolidation_score'].min()) / \n",
    "                                       (all_data['smoothed_consolidation_score'].max() - all_data['smoothed_consolidation_score'].min())) * (max_close - min_close) + min_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Parameter: Adjust consolidation threshold as needed\n",
    "non_consolidating = (all_data['consolidation_score'] <= 0.1)  \n",
    "\n",
    "fig, (ax1, ax3) = plt.subplots(2, 1, figsize=(14, 10), gridspec_kw={'height_ratios': [3, 1]})\n",
    "\n",
    "# Plotting the close price and volume\n",
    "color = 'tab:red'\n",
    "ax1.plot(all_data.index, all_data['Close'], color=color, label='Close Price')\n",
    "ax1.set_ylabel('Close Price', color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Secondary y-axis for volume\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.bar(all_data.index, all_data['Volume'], color=color, alpha=0.3, label='Volume')\n",
    "ax2.set_ylabel('Volume', color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Consolidation score plotting\n",
    "color = 'tab:green'\n",
    "ax3.plot(all_data.index, all_data['consolidation_score'], color=color, label='Consolidation Score', linewidth=2)\n",
    "ax3.set_ylabel('Consolidation Score', color=color)\n",
    "ax3.set_xlabel('Date')\n",
    "\n",
    "# Highlight non-consolidating periods\n",
    "ax3.fill_between(all_data.index, 0, 1, where=non_consolidating, color='red', alpha=0.3, transform=ax3.get_xaxis_transform(), label='Non-Consolidating')\n",
    "\n",
    "# Formatting dates\n",
    "ax1.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "ax3.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "ax3.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "\n",
    "# Legends\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "ax3.legend(loc='upper left')\n",
    "\n",
    "plt.title('Close Price, Volume, and Consolidation Score Over Time')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adjust the trend score to account for the strength of the signals\n",
    "all_data['trend_score'] = 0\n",
    "\n",
    "delta = all_data['Close'].diff()\n",
    "gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "rs = gain / loss\n",
    "all_data['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "# Uptrend indicators\n",
    "all_data.loc[(all_data['ShortEMA'] > all_data['LongEMA']) & (all_data['MACD'] > all_data['signal']), 'trend_score'] += 1\n",
    "all_data.loc[all_data['RSI'] > 40, 'trend_score'] += 1  # Consider lowering the threshold for more sensitivity\n",
    "all_data.loc[all_data['ADX_15'] > 25, 'trend_score'] += 1  # Strong trend\n",
    "all_data.loc[all_data['ATR_Ratio'] > 1, 'trend_score'] += 1  # Higher current volatility vs. past\n",
    "\n",
    "# Downtrend indicators\n",
    "all_data.loc[(all_data['ShortEMA'] < all_data['LongEMA']) & (all_data['MACD'] < all_data['signal']), 'trend_score'] -= 1\n",
    "all_data.loc[all_data['RSI'] < 60, 'trend_score'] -= 1  # Consider raising the threshold for more sensitivity\n",
    "all_data.loc[all_data['ADX_15'] > 25, 'trend_score'] -= 1  # Strong trend\n",
    "all_data.loc[all_data['ATR_Ratio'] < 1, 'trend_score'] -= 1  # Lower current volatility vs. past\n",
    "\n",
    "# Normalize the trend score to keep it bounded between -1 and 1\n",
    "all_data['trend_score_normalized'] = all_data['trend_score'] / all_data['trend_score'].abs().max()\n",
    "all_data['smoothed_trend_score'] = all_data['trend_score_normalized'].rolling(window=10).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax3) = plt.subplots(2, 1, figsize=(14, 10), gridspec_kw={'height_ratios': [3, 1]})\n",
    "\n",
    "# Plotting the close price and volume\n",
    "color = 'tab:red'\n",
    "ax1.plot(all_data.index, all_data['Close'], color=color, label='Close Price')\n",
    "ax1.set_ylabel('Close Price', color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Secondary y-axis for volume\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(all_data.index, all_data['Volume'], color='blue', alpha=0.3, label='Volume')\n",
    "ax2.set_ylabel('Volume', color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "\n",
    "# Consolidation score plotting\n",
    "color = 'tab:green'\n",
    "ax3.plot(all_data.index, all_data['smoothed_trend_score'], color=color, label='Trend Score', linewidth=2)\n",
    "ax3.set_xlabel('Date')\n",
    "\n",
    "# Formatting dates\n",
    "ax1.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "ax3.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "ax3.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "colors = np.where(all_data['smoothed_trend_score'] > 0, 'green', np.where(all_data['smoothed_trend_score'] < 0, 'red', 'grey'))\n",
    "ax3.plot(all_data.index, all_data['smoothed_trend_score'], color='grey', label='Trend Score')  # base line in grey\n",
    "ax3.fill_between(all_data.index, 0, all_data['smoothed_trend_score'], where=all_data['smoothed_trend_score'] >=0, facecolor='green', alpha=0.3, interpolate=True)\n",
    "ax3.fill_between(all_data.index, 0, all_data['smoothed_trend_score'], where=all_data['smoothed_trend_score'] <=0, facecolor='red', alpha=0.3, interpolate=True)\n",
    "\n",
    "# Legends\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "ax3.legend(loc='upper left')\n",
    "\n",
    "plt.title('Close Price, Volume, and Trend Score Over Time')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to keep, will optimize this\n",
    "features = ['ATR_Ratio', 'MACD', 'ADX_15', 'consolidation_score', 'trend_score', 'volume_score']\n",
    "data_selected = all_data[features]\n",
    "\n",
    "# Handles missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data_imputed = imputer.fit_transform(data_selected)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_imputed)\n",
    "\n",
    "# Convert scaled data back to DataFrame, ensuring it keeps the original index\n",
    "data_for_clustering = pd.DataFrame(data_scaled, columns=features, index=all_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Using KMeans for comparison\n",
    "range_n_clusters = range(2, 6)\n",
    "elbow = []\n",
    "silhouette = []\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(data_scaled)\n",
    "    \n",
    "    # Elbow method\n",
    "    elbow.append(clusterer.inertia_)\n",
    "    \n",
    "    # Silhouette score\n",
    "    silhouette_avg = silhouette_score(data_scaled, cluster_labels)\n",
    "    silhouette.append(silhouette_avg)\n",
    "\n",
    "# Plotting the results for KMeans\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range_n_clusters, elbow, 'bo-')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Sum of Squared Distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range_n_clusters, silhouette, 'ro-')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score For Optimal k')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume data_scaled is your pre-processed data\n",
    "range_n_clusters = range(2, 6)  # Typically 2 to 10 clusters\n",
    "\n",
    "# Variables to store results\n",
    "silhouette_scores = []\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    clusterer = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    cluster_labels = clusterer.fit_predict(data_scaled)\n",
    "    \n",
    "    # Silhouette score\n",
    "    silhouette_avg = silhouette_score(data_scaled, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "# Plotting the Silhouette scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range_n_clusters, silhouette_scores, 'ro-')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score For Optimal k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply clustering\n",
    "clustering = AgglomerativeClustering(n_clusters = 3)\n",
    "clustering.fit(data_for_clustering)\n",
    "\n",
    "# Attach clustering labels back to the original DataFrame\n",
    "all_data['cluster_labels'] = clustering.labels_\n",
    "\n",
    "# Check if indices and data length match\n",
    "print(all_data.shape)\n",
    "print(data_for_clustering.shape)\n",
    "print(\"Clustering labels added:\", all_data['cluster_labels'].isnull().sum() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Plot the close prices\n",
    "ax.plot(all_data.index, all_data['Close'], label='Close Price', color='blue')\n",
    "\n",
    "# Define a color map or specific colors for clusters\n",
    "colors = ['red', 'green', 'blue']  \n",
    "\n",
    "# Fill background according to cluster assignment, using a loop\n",
    "for i, color in enumerate(colors):  # Skip the last color, which we use for unclassified data\n",
    "    ax.fill_between(all_data.index, all_data['Close'].min(), all_data['Close'].max(),\n",
    "                    where=(all_data['cluster_labels'] == i),\n",
    "                    facecolor=color, alpha=0.2, label=f'Cluster {i}')\n",
    "\n",
    "# Adjust the y-axis limits to focus on the close price range\n",
    "ax.set_ylim(all_data['Close'].min() * 0.95, all_data['Close'].max() * 1.05)  # Adjust these factors as needed\n",
    "\n",
    "# Formatting the date axis\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "\n",
    "# Adding labels and legend\n",
    "ax.set_title('Close Price with Cluster Label Shading')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Close Price')\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Ensures 'Week' column is added correctly\n",
    "if not isinstance(all_data.index, pd.PeriodIndex):\n",
    "    all_data['Week'] = all_data.index.to_period('W')\n",
    "else:\n",
    "    all_data['Week'] = all_data.index\n",
    "\n",
    "# Verify 'Week' column exists\n",
    "print(all_data['Week'].head())\n",
    "\n",
    "# Select only numeric columns for aggregation\n",
    "numeric_data = all_data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Group by 'Week'  for smoothening\n",
    "weekly_data = numeric_data.groupby(all_data['Week']).mean()\n",
    "\n",
    "# Features to keep for weekly aggregated data, will optimize this\n",
    "features = ['ATR_Ratio', 'MACD', 'ADX_15', 'consolidation_score', 'smoothed_trend_score',  'volume_score']\n",
    "weekly_selected = weekly_data[features]\n",
    "\n",
    "# Handling missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "weekly_imputed = imputer.fit_transform(weekly_selected)\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "weekly_scaled = scaler.fit_transform(weekly_imputed)\n",
    "\n",
    "# Clustering\n",
    "clustering = AgglomerativeClustering(n_clusters=4)  # Reduced number of clusters\n",
    "clustering.fit(weekly_scaled)\n",
    "\n",
    "# Assign clusters back to the original daily data for plotting\n",
    "all_data['Cluster'] = np.repeat(clustering.labels_, all_data.groupby('Week').size().values)\n",
    "\n",
    "# Plotting setup\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.plot(all_data.index, all_data['Close'], label='Close Price', color='blue')\n",
    "\n",
    "colors = ['red', 'green', 'blue','purple']  # Adjust colors as needed\n",
    "for i, color in enumerate(colors):\n",
    "    ax.fill_between(all_data.index, all_data['Close'].min(), all_data['Close'].max(),\n",
    "                    where=(all_data['Cluster'] == i),\n",
    "                    facecolor=color, alpha=0.2, label=f'Cluster {i}')\n",
    "\n",
    "ax.set_ylim(all_data['Close'].min() * 0.95, all_data['Close'].max() * 1.05)\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Looped version (for when there are multiple tickers)\n",
    "import datetime\n",
    "finviz_url = 'https://finviz.com/quote.ashx?t='\n",
    "news_tables = {}\n",
    "daynum = 60\n",
    "tod = datetime.datetime.now()\n",
    "d = datetime.timedelta(days = daynum)\n",
    "a = tod - d\n",
    "\n",
    "df_new = mid.rename_axis('Ticker').head(24)\n",
    "tickers = df_new.index.tolist()\n",
    "\n",
    "charts = []\n",
    "n = 3\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        url = finviz_url + ticker\n",
    "        req = Request(url=url,headers={'user-agent': 'my-app/0.0.1'}) \n",
    "        resp = urlopen(req)    \n",
    "        html = BeautifulSoup(resp, features=\"lxml\")\n",
    "        news_table = html.find(id='news-table')\n",
    "        news_tables[ticker] = news_table\n",
    "        charts.append(str(url))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    for ticker in tickers:\n",
    "        df1 = news_tables[ticker]\n",
    "        df_tr = df1.findAll('tr')\n",
    "        display (df1)\n",
    "        display (df_tr)\n",
    "        print ('\\n')\n",
    "        print ('Recent News Headlines for {}: '.format(ticker))\n",
    "        for i, table_row in enumerate(df_tr):\n",
    "            a_text = table_row.a.text\n",
    "            td_text = table_row.td.text\n",
    "            td_text = td_text.strip()\n",
    "            print(a_text,'(',td_text,')')\n",
    "            if i == n-1:\n",
    "                break\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "# Iterate through the news\n",
    "parsed_news = []\n",
    "for file_name, news_table in news_tables.items():\n",
    "    for x in news_table.findAll('tr'):\n",
    "        text = x.a.get_text() \n",
    "        date_scrape = x.td.text.split()\n",
    "\n",
    "        if len(date_scrape) == 1:\n",
    "            time = date_scrape[0]\n",
    "        else:\n",
    "            date = date_scrape[0]\n",
    "            time = date_scrape[1]\n",
    "\n",
    "        ticker = file_name.split('_')[0]\n",
    "        parsed_news.append([ticker, date, time, text])\n",
    "\n",
    "# Sentiment Analysis\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "columns = ['Ticker', 'Date', 'Time', 'Headline']\n",
    "news = pd.DataFrame(parsed_news, columns=columns)\n",
    "scores = news['Headline'].apply(analyzer.polarity_scores).tolist()\n",
    "\n",
    "df_scores = pd.DataFrame(scores)\n",
    "news = news.join(df_scores, rsuffix='_right')\n",
    "\n",
    "# View Data \n",
    "news['Date'] = pd.to_datetime(news.Date).dt.date\n",
    "\n",
    "unique_ticker = news['Ticker'].unique().tolist()\n",
    "news_dict = {name: news.loc[news['Ticker'] == name] for name in unique_ticker}\n",
    "\n",
    "values = []\n",
    "for ticker in tickers: \n",
    "    try:\n",
    "        dataframe = news_dict[ticker]\n",
    "        dataframe = dataframe.set_index('Ticker')\n",
    "        dataframe = dataframe.drop(columns = ['Headline'])\n",
    "        print ('\\n')\n",
    "        print (dataframe.head())\n",
    "        mean = round(dataframe['compound'].mean(), 2)\n",
    "        values.append(mean)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "df1 = pd.DataFrame(list(zip(tickers, values, charts)), columns =['Ticker', 'Mean Sentiment', 'Chart']) \n",
    "df1 = df1.sort_values(by=['Mean Sentiment'], ascending=False)\n",
    "print(df1)\n",
    "# df1['Mean Sentiment'] = (df1['Mean Sentiment'] * 100) \n",
    "df1 = df1.astype({\"Mean Sentiment\": np.float16})\n",
    "df1['Mean Sentiment'] = df1.apply(lambda x : (1 + x['Mean Sentiment']) if (x['Mean Sentiment'] >= 0) else 1 - x['Mean Sentiment'], axis = 1)\n",
    "print ('\\n')\n",
    "print (df_new)\n",
    "\n",
    "new = pd.merge(df_new, df1, on = 'Ticker', how = 'outer')\n",
    "new = new.sort_values(by=['Mean Sentiment'], ascending=False)\n",
    "\n",
    "print (new)\n",
    "final_scores = new['Mean Sentiment'].tolist()\n",
    "final_tickers = new.Ticker.tolist()\n",
    "stocks = dict(zip(final_tickers, final_scores))\n",
    "\n",
    "new.to_excel( r'C:\\Users\\amoog\\Desktop\\Stock_Notifier\\Stock_Notifier\\spreadsheets\\{fdate}.xlsx'.format(fdate = 'Sentiment ' + tod.strftime(\"%d-%m-%Y\")), sheet_name = 'today', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
